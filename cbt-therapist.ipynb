{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10825931,"sourceType":"datasetVersion","datasetId":6722366}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install langchain_community","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fitz chromadb peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install frontend","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install pymupdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T13:35:54.029710Z","iopub.execute_input":"2025-02-23T13:35:54.030019Z","iopub.status.idle":"2025-02-23T13:35:59.279416Z","shell.execute_reply.started":"2025-02-23T13:35:54.029996Z","shell.execute_reply":"2025-02-23T13:35:59.278337Z"}},"outputs":[{"name":"stdout","text":"Collecting pymupdf\n  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.25.3\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install chromadb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install --upgrade transformers==4.41.0 torch==2.2.1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install accelerate>=0.27.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T13:56:35.637331Z","iopub.execute_input":"2025-02-23T13:56:35.637731Z","iopub.status.idle":"2025-02-23T13:56:39.152770Z","shell.execute_reply.started":"2025-02-23T13:56:35.637685Z","shell.execute_reply":"2025-02-23T13:56:39.151762Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Kaggle-specific installation (run first)\n!pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" \\\n    xformers==0.0.26 \\\n    \"huggingface_hub[cli]\" \\\n    --no-deps\n!pip install --force-reinstall torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 \\\n    --index-url https://download.pytorch.org/whl/cu121\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install unsloth vllm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\n# import fitz  # PyMuPDF for PDF text extraction\nimport torch\nimport chromadb\nfrom chromadb import Client, Settings\nfrom chromadb.utils import embedding_functions\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\nimport pymupdf \n# from unsloth import FastLanguageModel\n# Function to process PDFs from a folder.\ndef process_pdfs(pdf_folder):\n    documents = []\n    text= 'Hi'\n    for file in os.listdir(pdf_folder):\n        if file.endswith(\".pdf\"):\n            filepath = os.path.join(pdf_folder, file)\n            # with fitz.open(filepath) as doc:\n            #     text = \"\"\n            #     for i in range(len(doc)):\n            #         page = doc.load_page(i)\n            #         text += page.get_text(\"text\n            doc= pymupdf.open(filepath)\n            for page in doc: # iterate the document pages\n                  text += page.get_text() \n                # Split the extracted text into chunks (e.g., 500 characters each)\n            chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n            documents.extend(chunks)\n    return documents\n\n    doc = pymupdf.open(\"/kaggle/input/cbt-docs/narrative_therapy.pdf\") # open a document\n    for page in doc: # iterate the document pages\n      text += page.get_text() \n    file = open('narrative_therapy.txt', 'w') \n    file.write(text) \n    file.close()\n\n# Enhanced vector database with QIM integration.\nclass EnhancedTherapyDatabase:\n    def __init__(self):\n     \n        self.client = chromadb.PersistentClient(path=\".therapy_db\")\n        self.ef = embedding_functions.DefaultEmbeddingFunction()\n        self.collection = self.client.get_or_create_collection(\n            name=\"therapy_docs\",\n            embedding_function=self.ef\n        )\n        self.global_mean = None\n        self.global_std = None\n        self.bin_counts = {}\n\n    def add_documents(self, documents):\n        # Compute embeddings with the default embedding function.\n        embeddings = [self.ef([doc])[0] for doc in documents]\n        self.collection.add(documents=documents,\n                            ids=[str(i) for i in range(len(documents))],\n                            embeddings=embeddings)\n        # Compute global statistics (mean, std) and bin counts used in QIM.\n        self.compute_global_stats(embeddings)\n\n    def compute_global_stats(self, embeddings):\n        all_emb = np.array(embeddings)\n        self.global_mean = np.mean(all_emb, axis=0)\n        self.global_std = np.std(all_emb, axis=0) + 1e-6  # avoid division by zero\n        # For each dimension, quantize the embedding to 4 bits (0-15) and count occurrences\n        for vec in embeddings:\n            quantized = (vec * 15).astype(np.uint8)\n            for i, val in enumerate(quantized):\n                key = (i, int(val))\n                self.bin_counts[key] = self.bin_counts.get(key, 0) + 1\n\n    def qim_score(self, query_embed, doc_embed):\n        # Convert embeddings to 4-bit representation\n        q_bin = (query_embed * 15).astype(np.uint8)\n        d_bin = (doc_embed * 15).astype(np.uint8)\n        score = 0.0\n        for i in range(len(query_embed)):\n            diff = (query_embed[i] - doc_embed[i]) ** 2\n            key = (i, int(d_bin[i]))\n            N_i = self.bin_counts.get(key, 1)\n            score += diff * (N_i ** 2)\n        # Normalize score using the average global standard deviation\n        score = score / (len(query_embed) * np.mean(self.global_std))\n        return score\n\n    def query_qim(self, query_text, n_results=3):\n        query_embed = self.ef([query_text])[0]\n        all_docs = self.collection.get(include=[\"embeddings\", \"documents\", \"ids\"])\n        scores = []\n        for doc_id, doc_embed, doc_text in zip(all_docs[\"ids\"],\n                                                 all_docs[\"embeddings\"],\n                                                 all_docs[\"documents\"]):\n            # Compute the QIM score between query and document embedding\n            score = self.qim_score(np.array(query_embed), np.array(doc_embed))\n            scores.append((score, doc_text))\n        # Rank documents by descending QIM score (higher scores denote higher influence)\n        top_docs = sorted(scores, key=lambda x: x[0], reverse=True)[:n_results]\n        return \"\\n\".join([doc for score, doc in top_docs])\n\n# Function to load a quantized, fine-tuned LLM using QLoRA.\ndef load_llm(model_name):\n    # quant_config = BitsAndBytesConfig(\n    #     load_in_4bit=True,\n    #     bnb_4bit_quant_type=\"nf4\",\n    #     bnb_4bit_compute_dtype=torch.float16,\n    #     bnb_4bit_use_double_quant=True\n    # )\n    # lora_config = LoraConfig(\n    #     r=64,                 # attention dimension for LoRA\n    #     lora_alpha=16,        # scaling factor\n    #     target_modules=[\"q_proj\", \"v_proj\"],\n    #     lora_dropout=0.1,\n    #     bias=\"none\",\n    #     task_type=\"CAUSAL_LM\"\n    # )\n    # tokenizer = AutoTokenizer.from_pretrained(model_name, load_in_4bit=True)\n    # model = AutoModelForCausalLM.from_pretrained(model_name,\n    #                                              quantization_config=quant_config,\n    #                                              device_map=\"auto\", trust_remote_code=True)\n    # model = get_peft_model(model, lora_config)\n    # model.eval()\n    # return tokenizer, model\n    model, tokenizer = AutoModelForCausalLM.from_pretrained(\n        model_name, # Pre-quantized\n        max_seq_length = 2048,\n        dtype = torch.float16,\n        load_in_4bit = True,\n        device_map = \"auto\",\n        trust_remote_code= True\n    )\n    \n    # 2. Disable complex features for Kaggle compatibility \n    model.config.pretraining_tp = 1 # Disable tensor parallelism\n    model.config.use_cache = False\n\n# Therapy session class that combines context from the QIM database with LLM generation.\nclass TherapySession:\n    def __init__(self, db, tokenizer, model):\n        self.db = db\n        self.tokenizer = tokenizer\n        self.model = model\n        self.chat_history = []\n    \n    def get_context(self, user_input):\n        # Retrieve context from the vector store using QIM ranking.\n        context = self.db.query_qim(user_input, n_results=3)\n        return context\n\n    def generate_response(self, user_input):\n        context = self.get_context(user_input)\n        # Build a prompt incorporating therapeutic context, conversation history, and current user input.\n        prompt = (\n            \"You are a compassionate therapy assistant for war victims struggling with trauma. \"\n            \"Use the following context extracted from therapeutic literature to guide your answer:\\n\"\n            f\"{context}\\n\\n\"\n            \"Conversation History:\\n\"\n        )\n        for turn in self.chat_history:\n            prompt += f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\\n\"\n        prompt += f\"User: {user_input}\\nAssistant:\"\n\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        output = self.model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.7)\n        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n        if \"Assistant:\" in response:\n            response = response.split(\"Assistant:\")[-1].strip()\n        self.chat_history.append({\"user\": user_input, \"assistant\": response})\n        return response\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T14:41:04.082715Z","iopub.execute_input":"2025-02-23T14:41:04.083063Z","iopub.status.idle":"2025-02-23T14:41:04.100603Z","shell.execute_reply.started":"2025-02-23T14:41:04.083036Z","shell.execute_reply":"2025-02-23T14:41:04.099685Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"\n# Process PDFs from the designated folder (e.g., texts related to therapy for war victims)\npdf_folder = \"./therapy_pdfs\"  # ensure your PDFs are placed here\ndocuments = process_pdfs('/kaggle/input/cbt-docs')\nprint(\"Processed documents from PDFs.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the enhanced vector DB with QIM scoring.\ndb = EnhancedTherapyDatabase()\ndb.add_documents(documents)\nprint(\"Added documents to therapy database and computed global statistics.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the quantized fine-tuned LLM (the model should be available locally to run under low network conditions).\nmodel_name  = \"unsloth/DeepSeek-R1-GGUF\"  # replace with your local/fine-tuned model identifier\ntokenizer, model = load_llm(model_name)\nprint(\"Loaded quantized LLM model.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Start the interactive therapy session.\nsession = TherapySession(db, tokenizer, model)\nprint(\"Therapy assistant: Hello, I'm here to support you. How are you feeling today?\")\n\nwhile True:\n    try:\n        user_input = input(\"You: \")\n        if user_input.lower() in [\"exit\", \"quit\"]:\n            print(\"Therapy assistant: Thank you for sharing. Remember that you're always supported.\")\n            break\n        response = session.generate_response(user_input)\n        print(f\"Therapy assistant: {response}\")\n    except KeyboardInterrupt:\n        print(\"\\nTherapy assistant: Take care. I am always here if you need to talk.\")\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trauma_bot_deepseek.py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport torch\n\n# 1. Quantized Model Setup\nmodel_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n# )\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    # quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# 2. Custom PDF Processing\ndef process_pdfs(pdf_paths):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=512,\n        chunk_overlap=50\n    )\n    \n    docs = []\n    for path in pdf_paths:\n        loader = PyPDFLoader(path)\n        docs.extend(loader.load_and_split(text_splitter))\n    \n    return FAISS.from_documents(docs, HuggingFaceEmbeddings())\n\n# 3. Memory-Enhanced Generation\n# memory = ConversationBufferMemory()\nvector_store = process_pdfs([\"/kaggle/input/cbt-docs/2017-01-08_07-23-35_essays_on_war_traumas_adaptation_and_rehabilitation.pdf\", \n                                   \"/kaggle/input/cbt-docs/TF-CBT-toolkit-with-color-laminates-1.30.12.pdf\",\n                                 '/kaggle/input/cbt-docs/Your-Very-Own-TF-CBT-Workbook-Final.pdf',\n                                  '/kaggle/input/cbt-docs/cbt.pdf',\n                                  '/kaggle/input/cbt-docs/narrative_therapy.pdf'])\n\ndef generate_response(question):\n    # Retrieve relevant context\n    docs = vector_store.similarity_search(question, k=3)\n    context = \"\\n\".join([d.page_content for d in docs])\n    \n    # Memory integration\n    # history = memory.load_memory_variables({})['history']\n    \n    # Trauma-focused prompt template\n    # Conversation history:\n    # {history}\n    prompt = f\"\"\"<｜begin▁of▁sentence｜>You are a trauma specialist AI. Use this context:\n    {context}\n    \n    \n    \n    Current query: {question}\n    \n    Apply these steps:\n    1. Validate emotional state\n    2. Suggest CBT technique from manuals\n    3. Offer crisis resources if needed\n    Response:\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=500,\n        temperature=0.7,\n        repetition_penalty=1.1\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    memory.save_context({\"input\": question}, {\"output\": response})\n    \n    return response.split(\"Response:\")[-1].strip()\n\n# Example usage\nprint(generate_response(\"How do I handle survivor's guilt after losing family?\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vector_store = process_pdfs([\n                                   \"/kaggle/input/cbt-docs/TF-CBT-toolkit-with-color-laminates-1.30.12.pdf\",\n                                 '/kaggle/input/cbt-docs/Your-Very-Own-TF-CBT-Workbook-Final.pdf'\n                                \n                            ])\n\ndef generate_response(question):\n    # Retrieve relevant context\n    docs = vector_store.similarity_search(question, k=3)\n    context = \"\\n\".join([d.page_content for d in docs])\n    \n    # Memory integration\n    # history = memory.load_memory_variables({})['history']\n    \n    # Trauma-focused prompt template\n    # Conversation history:\n    # {history}\n    prompt = f\"\"\"<｜begin▁of▁sentence｜>You are a trauma specialist AI. Use this context:\n    {context}\n    \n    \n    \n    Current query: {question}\n    \n    Apply these steps:\n    1. Validate emotional state\n    2. Suggest CBT technique from manuals\n    3. Offer crisis resources if needed\n    Response:\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=500,\n        temperature=0.7,\n        repetition_penalty=1.1\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # memory.save_context({\"input\": question}, {\"output\": response})\n    \n    return response.split(\"Response:\")[-1].strip()\n\n# Example usage\nprint(generate_response(\"How do I handle survivor's guilt after losing family?\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:41:16.231059Z","iopub.execute_input":"2025-02-22T21:41:16.231444Z","iopub.status.idle":"2025-02-22T21:56:13.603715Z","shell.execute_reply.started":"2025-02-22T21:41:16.231411Z","shell.execute_reply":"2025-02-22T21:56:13.602614Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-10-ed07debecfcc>:35: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n  return FAISS.from_documents(docs, HuggingFaceEmbeddings())\nSetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"It sounds like you may be experiencing feelings of survivor's guilt after losing a loved one. This can be a common response to traumatic events, especially when individuals feel responsible for the outcome or believe they could have done something differently to prevent the loss. To cope with these emotions, please remember that there are several cognitive-behavioral techniques that you can try:\n\n1. Acknowledge and validate your emotions - Recognize that feeling guilty about surviving while others did not is a normal part of grief and trauma. Allow yourself time to process these feelings without judgment.\n\n2. Challenge negative thoughts using Cognitive Restructuring (CR): Survivor’s guilt often involves blaming oneself for the loss. Work through these thoughts by identifying any irrational beliefs and replacing them with more balanced perspectives. For example, consider whether it's reasonable to assume you should have been able to predict or control the situation.\n\n3. Practice self-compassion: Remind yourself that everyone reacts differently to tragic situations, and comparing yourself negatively won't help alleviate your guilt. Focus instead on taking care of yourself emotionally, physically, and spiritually during this difficult time.\n\n4. Reach out for support: Connect with friends and family members who understand your experience, as well as mental health professionals trained in treating trauma and related conditions such as post-traumatic stress disorder (PTSD). They can provide guidance, reassurance, and coping strategies tailored to your needs.\n\n5. Engage in healthy activities: Participate in regular physical exercise, eat nutritious meals, get enough sleep, and engage in hobbies or interests that bring joy and fulfillment. These practices contribute to overall mental and physical wellbeing, which can make it easier to navigate challenging emotions like guilt.\n\n6. Consider seeking therapy: If you find it difficult to manage your emotions independently, consult a mental health professional experienced in dealing with trauma and its aftermath. Together, you can explore therapeutic approaches that might aid in processing your experiences and helping you heal.\n\nAdditionally, if you need immediate assistance or wish to connect with resources specifically designed for survivors of traumatic events, refer to organizations such as the National Suicide Prevention Lifeline at 1-800-273-TALK (8255) or Crisis Text Line by texting HOME to 741741.\n","output_type":"stream"}],"execution_count":16}]}